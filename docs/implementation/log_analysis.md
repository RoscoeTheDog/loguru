# Loguru Log Analysis Toolkit

*Comprehensive log analysis and metrics collection for loguru JSON and text log files*

## üéØ Overview

The loguru log analysis toolkit provides powerful capabilities for analyzing log files generated by loguru. It supports both JSON format (using `serialize=True`) and standard text format logs, offering both CLI and programmatic interfaces.

## üì¶ Features

### Core Analysis Capabilities
- **Multi-format Support**: Parse both JSON and text log files
- **Comprehensive Metrics**: Level distribution, error rates, performance timing
- **Pattern Detection**: Automatic error pattern recognition  
- **Time Analysis**: Hourly/daily distribution and trend analysis
- **Context Analysis**: Extra field usage and context patterns
- **Health Monitoring**: Log health scores and issue detection

### Interface Options
- **CLI Tool**: `analyze_logs.py` for command-line analysis
- **Programmatic API**: Import functions directly from loguru
- **Report Generation**: Multiple report formats (summary, time, error, context)

## üöÄ Quick Start

### CLI Usage

```bash
# Basic analysis
python analyze_logs.py application.log

# Filter by level
python analyze_logs.py --level ERROR application.log

# Pattern search
python analyze_logs.py --pattern "connection.*failed" application.log

# Generate comprehensive reports
python analyze_logs.py --all-reports application.log

# Save report to file
python analyze_logs.py --summary -o report.txt application.log

# Analyze multiple files
python analyze_logs.py structured.log readable.log
```

### Programmatic Usage

```python
from loguru import logger, analyze_log_file, quick_stats, check_health

# Quick statistics
stats = quick_stats("application.log")
print(stats)

# Health check
health = check_health("application.log")
print(f"Health Score: {health['health_score']}/100")

# Detailed analysis
results = analyze_log_file("application.log")
print(f"Total entries: {results['total_entries']}")
print(f"Error rate: {results['error_rate']:.1f}%")

# Error-focused analysis
from loguru import get_error_summary
errors = get_error_summary("application.log")
print(f"Errors: {errors['error_count']}")

# Performance analysis
from loguru import get_performance_summary
perf = get_performance_summary("application.log")
print(f"Average duration: {perf['avg_duration']:.3f}s")
```

## üìä Analysis Reports

### Summary Report
```
LOG ANALYSIS SUMMARY
==================================================
Total Entries: 1,234
Time Range: 2025-08-26 08:00:00 to 2025-08-26 18:30:00
Duration: 10:30:00

LEVEL DISTRIBUTION:
------------------------------
     DEBUG:    456 ( 37.0%)
      INFO:    567 ( 46.0%)
   WARNING:    123 ( 10.0%)
     ERROR:     78 (  6.3%)
  CRITICAL:     10 (  0.8%)

TOP MODULES:
--------------------
  auth_service: 234
  database: 189
  api_handler: 156

PERFORMANCE METRICS:
-------------------------
  Average duration: 0.145s
  Median duration: 0.089s
  Max duration: 2.345s
```

### Time Analysis Report
```
TIME ANALYSIS
==============================

HOURLY DISTRIBUTION:
-------------------------
  08:00 |################################## 156
  09:00 |####################### 98
  10:00 |############################ 123
  ...

DAILY DISTRIBUTION:
-------------------------
  2025-08-26: 1,234
  2025-08-25: 1,456
  2025-08-24: 987
```

### Error Analysis Report
```
ERROR ANALYSIS
==============================
Total Errors: 88
Error Rate: 7.13%

EXCEPTION TYPES:
--------------------
  ConnectionError: 23
  ValueError: 18
  TimeoutError: 15

ERROR PATTERNS:
--------------------
  database connection failed: 23
  authentication timeout: 12
  invalid request format: 8
```

## üõ† API Reference

### Core Analysis Functions

#### `analyze_log_file(file_path)`
Comprehensive analysis of a single log file.

**Returns:**
```python
{
    'total_entries': int,
    'level_counts': dict,
    'error_rate': float,
    'time_range': dict,
    'top_modules': dict,
    'performance': dict,
    'hourly_distribution': dict,
    'daily_distribution': dict
}
```

#### `analyze_log_files(file_paths)`
Combined analysis of multiple log files.

#### `quick_stats(file_path)`
Get essential statistics as a formatted string.

**Example output:**
```
"Total Entries: 1,234 | Error Rate: 7.1% | Time Range: ... | Top Module: auth"
```

#### `check_health(file_path)`
Perform health assessment of log file.

**Returns:**
```python
{
    'health_score': int,      # 0-100
    'status': str,           # 'healthy', 'warning', 'critical'  
    'issues': list,          # List of identified issues
    'total_entries': int,
    'error_rate': float
}
```

### Specialized Analysis Functions

#### `get_error_summary(file_path)`
Error-focused analysis including patterns and exception types.

#### `get_performance_summary(file_path)`
Performance metrics including timing statistics.

#### `find_log_patterns(file_path, pattern)`
Search for entries matching regex pattern.

#### `get_time_distribution(file_path, granularity='hour')`
Time-based entry distribution.

#### `generate_report(file_path, report_type='summary', output_file=None)`
Generate formatted text reports.

**Report Types:**
- `'summary'` - Overview with key metrics
- `'time'` - Time-based analysis  
- `'error'` - Error-focused analysis
- `'context'` - Context field analysis
- `'all'` - All report types combined

## üè• Health Monitoring

The health check system evaluates log files across multiple dimensions:

### Health Score Calculation
- **100-80**: Healthy - Low error rates, good performance
- **79-60**: Warning - Moderate issues detected
- **59-0**: Critical - High error rates or performance problems

### Health Indicators
- **Error Rate**: Percentage of ERROR/CRITICAL entries
- **Performance**: Average execution timing
- **Volume**: Total number of log entries
- **Patterns**: Recurring error patterns

### Example Health Assessment
```python
health = check_health("app.json")
print(f"Score: {health['health_score']}/100")
print(f"Status: {health['status']}")

for issue in health['issues']:
    print(f"- {issue}")
```

## üìÅ File Format Support

### JSON Format (loguru with serialize=True)
```json
{
  "text": "2025-08-26 12:00:00 | INFO | User logged in",
  "record": {
    "time": {"timestamp": 1756221513.443552},
    "level": {"name": "INFO", "no": 20},
    "message": "User logged in",
    "module": "auth",
    "function": "login",
    "extra": {"user_id": "alice123"}
  }
}
```

### Text Format (standard loguru output)
```
2025-08-26 12:00:00 | INFO     | auth            | User logged in
2025-08-26 12:00:01 | WARNING  | database        | Connection slow
2025-08-26 12:00:02 | ERROR    | api             | Request failed
```

## üîß CLI Command Reference

### Basic Commands
```bash
# Analyze single file
python analyze_logs.py app.json

# Multiple files
python analyze_logs.py app.json backup.log

# Specific report types
python analyze_logs.py --summary app.json
python analyze_logs.py --time-analysis app.json
python analyze_logs.py --error-analysis app.json
python analyze_logs.py --context-analysis app.json
python analyze_logs.py --all-reports app.json
```

### Filtering Options
```bash
# Filter by log level
python analyze_logs.py --level ERROR app.json
python analyze_logs.py --level WARNING app.json

# Filter by time range
python analyze_logs.py --start-time "2025-08-26 08:00:00" \
                      --end-time "2025-08-26 18:00:00" app.json

# Filter by message pattern
python analyze_logs.py --pattern "database.*error" app.json
python analyze_logs.py --pattern "user.*login" app.json

# Filter by module
python analyze_logs.py --module "auth_service" app.json
```

### Output Options
```bash
# Save to file
python analyze_logs.py --summary -o analysis_report.txt app.json

# Multiple output formats
python analyze_logs.py --all-reports -o full_report.txt app.json
```

## üìà Performance Metrics

### Timing Analysis
When loguru logs include execution timing (through templates or custom logging), the analyzer provides:

- **Average Duration**: Mean execution time
- **Median Duration**: 50th percentile timing  
- **Min/Max Duration**: Fastest and slowest operations
- **Standard Deviation**: Timing consistency
- **Slow Operations**: Count of operations exceeding thresholds

### Performance Categories
- **Fast Operations**: < 100ms
- **Normal Operations**: 100ms - 1s  
- **Slow Operations**: > 1s

## üéØ Use Cases

### Development & Debugging
```python
# Quick development health check
health = check_health("dev.json")
if health['status'] != 'healthy':
    print("Issues found:")
    for issue in health['issues']:
        print(f"- {issue}")

# Find specific error patterns
errors = find_log_patterns("dev.json", r"NullPointerException|AttributeError")
for error in errors[-5:]:  # Last 5 errors
    print(f"{error['timestamp']} - {error['message']}")
```

### Production Monitoring
```python
# Daily log analysis
results = analyze_log_file("production.json")
error_rate = results['error_rate']

if error_rate > 5.0:
    # Alert: High error rate
    send_alert(f"High error rate: {error_rate:.1f}%")

# Performance monitoring
perf = get_performance_summary("production.json")
if perf['avg_duration'] > 1.0:
    # Alert: Slow performance
    send_alert(f"Slow performance: {perf['avg_duration']:.2f}s avg")
```

### Log File Maintenance
```bash
# Analyze large log files
python analyze_logs.py --summary large_app.json

# Generate maintenance reports
python analyze_logs.py --all-reports -o monthly_report.txt *.json

# Check multiple application logs
python analyze_logs.py --error-analysis app1.json app2.json app3.json
```

## üîç Advanced Features

### Pattern Recognition
The analyzer automatically detects common patterns:

- **Error Patterns**: Recurring error message patterns
- **Context Fields**: Most frequently used extra fields
- **Module Activity**: Which modules generate the most logs
- **Time Patterns**: Peak logging hours and days

### Multi-File Analysis
```python
# Analyze multiple files together
results = analyze_log_files([
    "app.json",
    "background_tasks.json", 
    "api_requests.json"
])

print(f"Combined entries: {results['total_entries']}")
print(f"Overall error rate: {results['error_rate']:.1f}%")
```

### Custom Analysis
```python
# Custom filtering and analysis
from loguru._log_analyzer import LogAnalyzer

analyzer = LogAnalyzer()

# Filter entries by custom criteria
filtered_entries = analyzer.filter_entries(
    "app.json",
    level="ERROR",
    start_time=datetime(2025, 8, 26, 8, 0, 0),
    end_time=datetime(2025, 8, 26, 18, 0, 0),
    message_pattern=r"database.*timeout"
)

# Process filtered entries
for entry in filtered_entries:
    print(f"{entry.timestamp} - {entry.message}")
```

## üö¶ Best Practices

### Log Analysis Workflow
1. **Quick Check**: Use `quick_stats()` for immediate overview
2. **Health Assessment**: Run `check_health()` to identify issues  
3. **Deep Dive**: Use specific analysis functions based on findings
4. **Report Generation**: Create reports for stakeholders

### Performance Considerations
- Large files (>100MB): Use filtering options to focus analysis
- Multiple files: Consider analyzing in batches
- Real-time monitoring: Use programmatic API for automated checks

### Error Analysis Strategy
1. Start with error rate and patterns
2. Identify recurring exception types
3. Correlate errors with time patterns
4. Check context fields for additional insights

## üõ° Integration with Existing Systems

### Monitoring Systems
```python
# Integration with monitoring dashboards
def collect_log_metrics():
    health = check_health("production.json")
    
    metrics = {
        'log_health_score': health['health_score'],
        'log_error_rate': health['error_rate'],
        'log_entry_count': health['total_entries']
    }
    
    send_to_monitoring(metrics)
```

### Alerting Systems
```python
# Automated alerting based on log analysis
def check_and_alert():
    health = check_health("app.json")
    
    if health['status'] == 'critical':
        send_alert("Critical log issues detected", health['issues'])
    elif health['status'] == 'warning':
        send_warning("Log quality issues found", health['issues'])
```

### CI/CD Integration
```bash
# Add to CI/CD pipeline
python analyze_logs.py --error-analysis build_logs.json
if [ $? -ne 0 ]; then
    echo "Log analysis failed - build may have issues"
    exit 1
fi
```

## üìù Examples and Recipes

### Recipe: Daily Health Check
```python
import schedule
from loguru import check_health, get_error_summary

def daily_log_check():
    health = check_health("application.json")
    errors = get_error_summary("application.json")
    
    report = f"""
    Daily Log Health Report
    ======================
    Health Score: {health['health_score']}/100
    Status: {health['status']}
    Errors: {errors['error_count']}
    Error Rate: {errors['error_rate']:.1f}%
    
    Top Error Patterns:
    {errors['top_error_patterns']}
    """
    
    send_daily_report(report)

schedule.every().day.at("09:00").do(daily_log_check)
```

### Recipe: Performance Monitoring
```python
from loguru import get_performance_summary

def monitor_performance():
    perf = get_performance_summary("api_logs.json")
    
    if perf['avg_duration'] > 0.5:  # 500ms threshold
        slow_ops = perf['slow_operations']
        alert = f"Performance degradation detected: {perf['avg_duration']:.3f}s avg, {slow_ops} slow operations"
        send_performance_alert(alert)

# Run every hour
schedule.every().hour.do(monitor_performance)
```

### Recipe: Error Pattern Analysis
```python
from loguru import find_log_patterns, get_error_summary

def analyze_error_trends():
    # Get overall error summary
    errors = get_error_summary("logs.json")
    
    # Find specific patterns
    db_errors = find_log_patterns("logs.json", r"database.*error")
    auth_errors = find_log_patterns("logs.json", r"authentication.*failed")
    
    print(f"Database errors: {len(db_errors)}")
    print(f"Auth errors: {len(auth_errors)}")
    print(f"Error rate trend: {errors['error_rate']:.1f}%")
```

---

The loguru log analysis toolkit provides enterprise-grade log analysis capabilities while maintaining the simplicity and elegance that makes loguru popular. Whether you're debugging in development or monitoring production systems, these tools help you understand what your logs are telling you.